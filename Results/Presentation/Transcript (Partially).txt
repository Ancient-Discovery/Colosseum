(CLICK)
Convolutional Neural Network is the method that we use to recognise our ancient characters. It transfers the recognition to a classification problem. And this is how it works. We feed data here [POINT] and CNN uses filters to look for features; the extracted features will be fully mapping to multiple-level layers, and then to the result vector.

(CLICK)
This is the structure of our CNN.

(CLICK)
This table describes several experiments with different parameters. In the performance comparison figure below, we can see that each of accuracy rate will converge to around 70%. In the reasonable range, the more we train, the more layers we have, the more performance it will have. And we can also know that if we tune learning rate a little bit, it will change training results dramatically, like butterfly effect if you know what I mean.

(CLICK)
And here is our samples of CNN training. For row one and row four, the first two characters are actually wrongly matched, but even from human's point of view (which is me), they look so similar, which means our network runs in a very logic way.

(CLICK)
There are several issues to be fixed and our prioirity is the training speed. Typically, it takes hours, even days to build a runnable neural network, which is totally pain in the arse. And I know we can do some tweaks on algorithm itself, but we decided to work on our dataset instead. Here is what we did. We used thinning algorithm based on mathematical morphology to reduce the edge of binary images and still keep the whole structure of original images. And I don't want to be cocky or something, but geez after applying this method, training speed is WAY faster than previous ones, and more importantly, accuracy rate does not drop, at all.

(CLICK)
Now let meet our new friend called generative adversarial networks. There are hundreds of them available and we did a simple classification and based on that we selected two unique ones for our next phase.

(CLICK)
The first one is called Conditional GAN. "Conditional" here just means adding limits or boundaries to original GAN. And Pix2Pix is an improved version of Conditional GAN, which our code heavily depends on. It may sound really easy to use some of other's code to do your own project; as a matter of fact, it's not. The first main issue we met is that it simply does not work. All generated images are purely black; obviously the algorithm learnt the biggest feature shared by all images we have, single colour background. Therefore we have to create a background which is hard for machines to learn, even impossible. And here comes our simple solution, random noise. We chose the most effective one from 6 different noise generation alogrithms.

(CLICK)
And it is typical results of our first 200-epoch-run. As you see, the left hand side one is original image; right hand side one is the ideal image we want to generate and the one in the middle is what we get from Conditional GAN. Obviously, it looks disappointing so we kept training. After 5000 epochs, it looks better; you can see components begin to connect each other and start to become a complete symbol. And I know what you are thinking now. Ye, what about just keeping doing? We did. And here you go. TADAA! I am afraid to say that the result is even worse not only for the character but for all of them, which indicates somehow the limit of the alogritm when it comes to character generation.

(CLICK)
Here are other samples removing background noise to make them clear to see. And that's that; next we will introduce another type of generative adverserial network, which I promise you, will be much better.

KEYWORDS:
convoluitional neural networks
several experiments with different parameters
mathematical morphology
generative adversarial networks
oracle bone scripts
epochs
